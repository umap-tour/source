{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \\\n",
    "#     torch==1.4.0 \\\n",
    "#     torchvision==0.4.1 \\\n",
    "#     umap-learn==0.4.4\n",
    "#     tqdm==4.46.1\\\n",
    "#     matplotlib==3.1.1\\\n",
    "#     natsort==6.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import json\n",
    "import math\n",
    "import os,sys\n",
    "from pathlib import Path\n",
    "from random import sample, seed\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader \n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from natsort import natsorted\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-colorblind')\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##config\n",
    "dataset_dir = './dataset/imagenet/val'\n",
    "out_dir = './out'\n",
    "\n",
    "\n",
    "### real\n",
    "# ndim = 15\n",
    "# npoint = 50000\n",
    "# maxDim = 2048 * 5 * 5\n",
    "# maxTargetWidth = 16\n",
    "\n",
    "# ## test\n",
    "ndim = 2\n",
    "npoint = 1000\n",
    "maxDim = 100\n",
    "maxTargetWidth = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not Path(out_dir).exists():\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'Device: {device}')\n",
    "print(f'loading dataset from: {dataset_dir}')\n",
    "print(f'output directory: {out_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dummy layers\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.flatten(x, 1)\n",
    "    \n",
    "class Input(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveLabels(targets, out_dir):\n",
    "    print('Getting labels...')\n",
    "    fn_out = out_dir + '/labels.bin'\n",
    "    if not (Path(out_dir) / 'labels.bin').exists():\n",
    "        labels = np.array(targets)\n",
    "        labels.astype(np.uint16).tofile(fn_out)\n",
    "    else:\n",
    "        labels = np.fromfile(fn_out, dtype=np.uint16)\n",
    "        print('Skipped because labels.bin exists:', fn_out)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def getUmap(loader, modules, layerNames, \n",
    "    npoint, ndim, \n",
    "    maxDim, maxTargetWidth, \n",
    "    out_dir, xy_prev = None,\n",
    "    pooling='average',\n",
    "    logging=True,\n",
    "    seed=None\n",
    "):\n",
    "    f = None\n",
    "    if logging:\n",
    "        f = open(f'{out_dir}/log.txt', 'a')\n",
    "        def log(*args): \n",
    "            print(*args)\n",
    "            dt=datetime.now().strftime('[%Y-%m-%d %H:%M:%S] ')\n",
    "            f.write(dt + ' '.join([str(a) for a in args]) + '\\n')\n",
    "            f.flush()\n",
    "    else:\n",
    "        log = print\n",
    "        \n",
    "    maxBatchCount = math.ceil(npoint / loader.batch_size)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            log('Getting Activations...')\n",
    "            for i in range(0, len(modules)):\n",
    "                name = layerNames[i]\n",
    "                fn_out = out_dir + f'/{name}.bin'\n",
    "                if Path(fn_out).exists():\n",
    "                    log(f'skipped {name} because file exists: {fn_out}')\n",
    "                    xy_prev = np.fromfile(fn_out, dtype=np.float32).reshape([npoint, ndim])\n",
    "                    continue\n",
    "                else:\n",
    "                    module = modules[i]\n",
    "                    log(name, '...')\n",
    "                    t0 = time()\n",
    "                    acts = None\n",
    "                    total = 0\n",
    "                    for batchIndex, [imgs, targets] in enumerate(tqdm(loader)):\n",
    "                        act = module(imgs.to(device)).detach().cpu()\n",
    "                        if pooling is not None \\\n",
    "                        and len(act.size())==4 \\\n",
    "                        and act.view(act.size(0), -1).size(1) > maxDim:\n",
    "                            nchannel = act.size(1)\n",
    "                            target_width = math.floor(math.sqrt(maxDim / nchannel))\n",
    "                            target_width = min(target_width, maxTargetWidth)\n",
    "                            if target_width == 0:\n",
    "                                target_width = 1\n",
    "                            if pooling == 'average':\n",
    "                                pool = nn.AdaptiveAvgPool2d([target_width,target_width])\n",
    "                            elif pooling == 'max':\n",
    "                                pool = nn.AdaptiveMaxPool2d([target_width,target_width])\n",
    "                            else:\n",
    "                                pool = pooling\n",
    "                            act = pool(act)\n",
    "                        if acts is None:\n",
    "                            acts = torch.zeros(npoint, *act.shape[1:])\n",
    "                        if npoint-total > act.shape[0]:\n",
    "                            acts[total:total+act.shape[0]] = act\n",
    "                        else:#final batch\n",
    "                            acts[total:] = act[:npoint-total]\n",
    "                        total += act.shape[0]\n",
    "                        del act\n",
    "                        if total > npoint:\n",
    "                            break\n",
    "                    log(acts.size())\n",
    "                    log(f'activation done in {(time()-t0):.2f} sec')\n",
    "\n",
    "                    log('UMAP...')\n",
    "                    acts = acts.view(acts.size(0), -1)\n",
    "                    t0 = time()\n",
    "\n",
    "    #                 if xy_prev is not None:\n",
    "    #                     umap = UMAP(n_components=ndim, init=xy_prev)\n",
    "    #                 else:\n",
    "    #                     print('spectral init')\n",
    "    #                     umap = UMAP(n_components=ndim, init='spectral')\n",
    "                    \n",
    "                    umap = UMAP(n_components=ndim, init='spectral', random_state=seed)\n",
    "                    log(f'UMAP input size={acts.shape}')\n",
    "                    xy = umap.fit_transform(acts)\n",
    "                    xy.astype(np.float32).tofile(fn_out)\n",
    "                    log(f'UMAP done in {(time()-t0):.2f} sec')\n",
    "                    xy_prev = xy\n",
    "                log('=' * 80)\n",
    "    finally:\n",
    "        if f is not None:\n",
    "            f.close()\n",
    "\n",
    "\n",
    "def detail2coarse(dir0):\n",
    "    \n",
    "    fn_in = dir0 + '/labels.bin'\n",
    "    fn_out = dir0 + '/labels-coarse.bin'\n",
    "\n",
    "    with open('data/imagenet_class_index.json') as f:\n",
    "        index2wnidlabel = json.load(f)\n",
    "\n",
    "    coarse2detail = OrderedDict()\n",
    "    with open('data/imagenet_coarse_categories.csv') as f:\n",
    "        f.readline() ## skip the header\n",
    "        for line in f:\n",
    "            line = line.strip().split(',')\n",
    "            try:\n",
    "                stop = line.index('')\n",
    "                coarse2detail[line[0]] = line[1:stop]\n",
    "            except:\n",
    "                coarse2detail[line[0]] = line[1:]\n",
    "    \n",
    "    ## fix some labels\n",
    "    coarse2detail['vehicle'][8] = 'crane (machine)'\n",
    "    coarse2detail['clothing'][25] = 'maillot (tank suit)'\n",
    "    coarse_to_coarseIndex = {k:i for i,k in enumerate(coarse2detail.keys())}\n",
    "    detail_to_detailIndex = {v[1]:int(k) for k,v in index2wnidlabel.items()}\n",
    "    detail_to_detailIndex['crane (bird)'] = detail_to_detailIndex['crane'] ## fix\n",
    "\n",
    "    detail2coarse = {}\n",
    "    for c,ds in coarse2detail.items():\n",
    "        for d in ds:\n",
    "            detail2coarse[detail_to_detailIndex[d]] = coarse_to_coarseIndex[c]\n",
    "\n",
    "    labels = np.fromfile(fn_in, dtype=np.uint16)\n",
    "    labels_coarse = np.array([detail2coarse[l] for l in labels], dtype=np.uint16)\n",
    "\n",
    "    labels_coarse.tofile(fn_out)\n",
    "    print(fn_out)\n",
    "    \n",
    "    \n",
    "def saveSchema(out_dir, layerNames, npoint, ndim):\n",
    "    schema = OrderedDict()\n",
    "    schema['archIndex'] = 0\n",
    "    schema['labels'] = {\n",
    "    #     'id': 'labels',\n",
    "        'id': 'labels-coarse',\n",
    "        'shape': npoint,\n",
    "    }\n",
    "    schema['layers'] = []\n",
    "\n",
    "    for layerName in layerNames:\n",
    "        layerMeta = {\n",
    "            'id': layerName,\n",
    "            'shape': [npoint, ndim],\n",
    "#             'residual': 0 ## TODO\n",
    "        }\n",
    "        schema['layers'].append(layerMeta)\n",
    "\n",
    "    with open(out_dir + '/schema.json', 'w') as f:\n",
    "        json.dump(schema, f, indent=2)\n",
    "    print(out_dir + '/schema.json')\n",
    "    \n",
    "\n",
    "\n",
    "def procrustes_similarity(traveler, bed, centralize=True):\n",
    "    if centralize:\n",
    "        traveler -= traveler.mean(0)\n",
    "        bed -= bed.mean(0)\n",
    "    \n",
    "    if type(traveler).__name__ == 'ndarray':\n",
    "        # numpy\n",
    "        norm = np.linalg.norm\n",
    "    else:\n",
    "        # pytorch\n",
    "        norm = torch.linalg.norm\n",
    "    sim = norm(traveler.T @ bed, 'nuc') / (\n",
    "        norm(traveler.T @ traveler, 'nuc') * norm(bed.T @ bed, 'nuc')\n",
    "    )**0.5\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAttributes(loader, model, npoint, out_dir, device):\n",
    "    ### labels\n",
    "    with open('data/imagenet_class_index.json') as f:\n",
    "        index2wnidlabel = json.load(f)\n",
    "\n",
    "    coarse2detail = OrderedDict()\n",
    "    with open('data/imagenet_coarse_categories.csv') as f:\n",
    "        f.readline() ## skip the header\n",
    "        for line in f:\n",
    "            line = line.strip().split(',')\n",
    "            try:\n",
    "                stop = line.index('')\n",
    "                coarse2detail[line[0]] = line[1:stop]\n",
    "            except:\n",
    "                coarse2detail[line[0]] = line[1:]\n",
    "\n",
    "    coarse2detail['vehicle'][8] = 'crane (machine)'\n",
    "    coarse2detail['clothing'][25] = 'maillot (tank suit)'\n",
    "\n",
    "    coarse_to_coarseIndex = {k:i for i,k in enumerate(coarse2detail.keys())}\n",
    "    coarseIndex_to_coarse = {v:k for k,v in coarse_to_coarseIndex.items()}\n",
    "\n",
    "    detail_to_detailIndex = {v[1]:int(k) for k,v in index2wnidlabel.items()}\n",
    "    detail_to_detailIndex['crane (bird)'] = detail_to_detailIndex['crane'] ## fix\n",
    "\n",
    "    detail2coarse = {}\n",
    "    for c,ds in coarse2detail.items():\n",
    "        for d in ds:\n",
    "            detail2coarse[detail_to_detailIndex[d]] = coarse_to_coarseIndex[c]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## 'confidence' and error to the corect label given by Softmax layer\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    softmax = nn.Softmax(1)\n",
    "    ce = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "#     labels = [index2wnidlabel[str(i)][1] for i in dataset.targets]\n",
    "#     labels_coarse = [coarseIndex_to_coarse[detail2coarse[i]] for i in dataset.targets]\n",
    "    labels = []\n",
    "    labels_coarse = []\n",
    "    confidence = []\n",
    "    error = []\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for img, target in tqdm(loader):\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            out = model(img)\n",
    "            loss = ce(out, target)\n",
    "            pred = softmax(out)\n",
    "            conf = pred.gather(1, target.view(-1,1)).view(-1)\n",
    "            \n",
    "            for i in target:\n",
    "                i = i.item()\n",
    "                labels.append(index2wnidlabel[str(i)][1])\n",
    "                labels_coarse.append(coarseIndex_to_coarse[detail2coarse[i]])\n",
    "            error.append(loss.cpu())\n",
    "            confidence.append(conf.cpu())\n",
    "            total += img.size(0)\n",
    "            if total > npoint:\n",
    "                break\n",
    "    \n",
    "    labels = labels[:npoint]\n",
    "    labels_coarse = labels_coarse[:npoint]\n",
    "    error = torch.cat(error)[:npoint].numpy()\n",
    "    confidence = torch.cat(confidence)[:npoint].numpy()\n",
    "    \n",
    "    data = np.stack([\n",
    "        labels, labels_coarse, \n",
    "        confidence, error, \n",
    "    ], 1)\n",
    "    columns = [\n",
    "        'label', 'label_coarse', \n",
    "        'confidence', 'error', \n",
    "    ]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.to_csv(f'{out_dir}/attributes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'loading dataset from: {dataset_dir}', end='... ')\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "transform = T.Compose([\n",
    "    T.Resize(256), \n",
    "    T.CenterCrop(224), \n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "dataset = datasets.ImageFolder(dataset_dir, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'loading model', end='... ')\n",
    "\n",
    "modelName = 'googlenet'\n",
    "model = models.googlenet(pretrained=True, aux_logits=False)\n",
    "\n",
    "# modelName = 'resnet50'\n",
    "# model = models.resnet50(pretrained=True)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f'{modelName} with ~{sum(p.numel() for p in model.parameters())/1e6:.1f}M', 'parameters, DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define layers of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##resnet 50/101\n",
    "# modules = [\n",
    "#     Input(),\n",
    "#     model.conv1, \n",
    "#     model.bn1, \n",
    "#     model.relu, \n",
    "#     model.maxpool, \n",
    "#     *model.layer1,\n",
    "#     *model.layer2,\n",
    "#     *model.layer3,\n",
    "#     *model.layer4,\n",
    "#     model.avgpool,\n",
    "#     Flatten(),\n",
    "#     model.fc,\n",
    "#     nn.Softmax(dim=1)\n",
    "# ]\n",
    "\n",
    "\n",
    "##GoogLeNet (inception v1)\n",
    "modules = [\n",
    "    Input(),\n",
    "    model.conv1,\n",
    "    model.maxpool1,\n",
    "    model.conv2,\n",
    "    model.conv3,\n",
    "    model.maxpool2,\n",
    "    model.inception3a,\n",
    "    model.inception3b,\n",
    "    model.maxpool3,\n",
    "    model.inception4a,\n",
    "    model.inception4b,\n",
    "    model.inception4c,\n",
    "    model.inception4d,\n",
    "    model.inception4e,\n",
    "    model.maxpool4,\n",
    "    model.inception5a,\n",
    "    model.inception5b,\n",
    "    model.avgpool,\n",
    "    Flatten(),\n",
    "#     model.dropout,\n",
    "    model.fc,\n",
    "    nn.Softmax(dim=1)\n",
    "]\n",
    "\n",
    "\n",
    "name_template = '{}-{}'\n",
    "layerNames = [name_template.format(i, str(m).split('(')[0]) for i,m in enumerate(list(modules), 0)]\n",
    "modules_concat = [nn.Sequential(*modules[:i+1]) for i in range(len(modules))]\n",
    "print('layers:', layerNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show pooling dimensions\n",
    "print('Pooling dimensions:')\n",
    "\n",
    "\n",
    "\n",
    "test_input = torch.rand([1,3,224,224]).to(device)\n",
    "y = test_input\n",
    "for name, m in zip(layerNames, modules):\n",
    "    y = m(y)\n",
    "    print(name)\n",
    "    print(f'original -->', y.size())\n",
    "    if len(y.size()) == 4:\n",
    "        nchannel = y.size(1)\n",
    "        width = y.size(3)\n",
    "        if y.view(1,-1).size(1) > maxDim:\n",
    "            target_width = math.floor(math.sqrt(maxDim / nchannel))\n",
    "            target_width = min(maxTargetWidth, target_width)\n",
    "            if target_width == 0:\n",
    "                target_width = 1\n",
    "            global_pooling = nn.AdaptiveAvgPool2d([target_width, target_width])\n",
    "            y_pool = global_pooling(y)\n",
    "        else:\n",
    "            y_pool = y\n",
    "        print('  pooled -->', y_pool.size())\n",
    "        print(' flatten -->', y_pool.view(1,-1).size())\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startLayer = 0\n",
    "stopLayer = len(modules)\n",
    "\n",
    "out_subdir = f'{out_dir}/umap-{modelName}-{npoint}x{ndim}D/'\n",
    "if not Path(out_subdir).exists():\n",
    "    os.makedirs(out_subdir)\n",
    "print('Output dir:', out_subdir)\n",
    "\n",
    "saveLabels(dataset.targets, out_subdir)\n",
    "detail2coarse(out_subdir)\n",
    "\n",
    "getUmap(loader, \n",
    "        modules_concat[startLayer:stopLayer], \n",
    "        layerNames[startLayer:stopLayer], \n",
    "        npoint, ndim, maxDim, maxTargetWidth, \n",
    "        out_subdir,\n",
    "        seed=42)\n",
    "saveSchema(out_subdir, layerNames, npoint, ndim)\n",
    "saveAttributes(loader, model, npoint, out_subdir, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save image tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageSize = 32\n",
    "nrow = 100 ## num of image per row\n",
    "ncol = 100\n",
    "\n",
    "\n",
    "dir_in = dataset_dir\n",
    "dir_out = f'{out_dir}/images'\n",
    "if not Path(dir_out).exists():\n",
    "    os.makedirs(dir_out)\n",
    "    \n",
    "transform = T.Compose([\n",
    "    T.Resize(256), \n",
    "    T.CenterCrop(224),\n",
    "    T.Resize(imageSize), \n",
    "    T.ToTensor(),\n",
    "])\n",
    "dataset = datasets.ImageFolder(dir_in, transform=transform)\n",
    "image_loader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "maxBatchCount = math.ceil(npoint / loader.batch_size)\n",
    "\n",
    "imagePerPatch = nrow*ncol\n",
    "\n",
    "all_imgs = []\n",
    "for batchIndex, [imgs, targets] in enumerate(tqdm(image_loader)):\n",
    "    all_imgs.append(imgs)\n",
    "    if batchIndex >= maxBatchCount:\n",
    "        break\n",
    "all_imgs = torch.cat(all_imgs)[:npoint]\n",
    "\n",
    "patchCount = math.ceil(npoint/imagePerPatch)\n",
    "for i in tqdm(range(patchCount)):\n",
    "    grid = make_grid(all_imgs[i*imagePerPatch:(i+1)*imagePerPatch], nrow=nrow, padding=0)\n",
    "    if i < patchCount-1:\n",
    "        canvas = grid\n",
    "    else:\n",
    "        canvas = torch.zeros([3, ncol*imageSize, nrow*imageSize])\n",
    "        canvas[:, :grid.shape[1], :grid.shape[2]] = grid\n",
    "    save_image(canvas, f'{dir_out}/imagenet-{imageSize}-{ncol}x{nrow}-{npoint}-{i}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Similairy matrix using UMAP+Procrustes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelName1 = 'inceptionv3'\n",
    "# modelName2 = 'vgg16'\n",
    "\n",
    "# dir1 = \n",
    "# dir2 = \n",
    "# fns1 = natsorted([f for f in glob(f'{dir1}/*.bin') if 'labels' not in f])\n",
    "# fns2 = natsorted([f for f in glob(f'{dir2}/*.bin') if 'labels' not in f])\n",
    "# fns1, fns2\n",
    "\n",
    "### npoint, ndim = 50000, 15\n",
    "# proMatrix = np.zeros([len(fns1),len(fns2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i, fn_i in enumerate(tqdm(fns1)):\n",
    "#     act_i = np.fromfile(fn_i, dtype=np.float32).reshape([-1, ndim])\n",
    "#     for j, fn_j in enumerate(fns2):\n",
    "#         if proMatrix[i,j] > 0:## if already computed\n",
    "#             continue\n",
    "#         else:\n",
    "#             act_j = np.fromfile(fn_j, dtype=np.float32).reshape([-1, ndim])\n",
    "#             proMatrix[i,j] = procrustes_similarity(\n",
    "#                 act_i, \n",
    "#                 act_j, \n",
    "#                 centralize=True\n",
    "#             )\n",
    "# fn = f'data/vis/similarity-matrix/sim-{modelName1}-vs-{modelName2}-{len(fns1)}x{len(fns2)}.bin'\n",
    "# proMatrix.astype(np.float32).tofile(fn)\n",
    "\n",
    "# plt.imshow(proMatrix)\n",
    "# print(fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
